{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "NdFKX9hFGvJe"
   },
   "source": [
    "# DS 2021"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "8w8bDO-gGymt"
   },
   "source": [
    "사과 이미지를 (64,64) 크기로 resize 한 후 autoendoer에 입력하는 프로그램 코드다. 이를 참고하여 다음 조건을 갖추도록 아래 코드를 수정하여, 실행하고, 저장한 후 제출하시오.  \n",
    "아래 조건과 관련 없는 부분은 수정하지 마세요."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "w5WY9pIEGyvh"
   },
   "source": [
    "latent dimension은 5로 한다.  \n",
    "주어진 입력 이미지를 (32,32)로 resize 한 후에 사용하여야 한다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "Collapsed": "false",
    "id": "LKrL0-f8Nev2"
   },
   "outputs": [],
   "source": [
    "from google.colab import drive\n",
    "drive.mount('/content/drive')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "Collapsed": "false",
    "id": "NFXKHTT9NgRv"
   },
   "outputs": [],
   "source": [
    "cd drive/My Drive/.... # 본인의 google drive 폴더에 맞게 수정하세요."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "Collapsed": "false",
    "id": "cuhNxv51NL_K"
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.autograd import Variable\n",
    "\n",
    "import torchvision\n",
    "from torchvision import datasets\n",
    "from torchvision import transforms\n",
    "from torchvision.utils import save_image\n",
    "from torchsummary import summary\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "#from pushover import notify\n",
    "#from utils import makegif\n",
    "from random import randint\n",
    "\n",
    "from IPython.display import Image\n",
    "from IPython.core.display import Image, display\n",
    "\n",
    "#%load_ext autoreload\n",
    "#%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "Collapsed": "false",
    "id": "EBcQsmu9NL_R"
   },
   "outputs": [],
   "source": [
    "batch_size = 32\n",
    "num_epochs= 50"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "Collapsed": "false",
    "id": "6Trl-9_MNL_Q"
   },
   "outputs": [],
   "source": [
    "# Device configuration\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "Collapsed": "false",
    "id": "ddT_0e1_klUZ"
   },
   "outputs": [],
   "source": [
    "trans = transforms.Compose([\n",
    "    transforms.Resize((64,64)),\n",
    "    transforms.ToTensor()\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "qUJLKGw9MqmE"
   },
   "outputs": [],
   "source": [
    "train_data = torchvision.datasets.ImageFolder(root='train', transform=trans)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "t_PI4xbPMo93"
   },
   "outputs": [],
   "source": [
    "train_loader = DataLoader(dataset = train_data, batch_size = batch_size, shuffle = True, num_workers=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "vEpiHlwcNxaG"
   },
   "outputs": [],
   "source": [
    "len(train_data.imgs),len(train_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "Collapsed": "false",
    "id": "_DJU2_-CNL_S"
   },
   "outputs": [],
   "source": [
    "# Fixed input for debugging\n",
    "fixed_x, _ = next(iter(train_loader))\n",
    "print(fixed_x.shape)\n",
    "save_image(fixed_x, 'real_image.png')\n",
    "\n",
    "Image('real_image.png')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "Collapsed": "false",
    "id": "IWX6quCNNL_S"
   },
   "outputs": [],
   "source": [
    "class Flatten(nn.Module):\n",
    "    def forward(self, input):\n",
    "        return input.view(input.size(0), -1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "Collapsed": "false",
    "id": "88r3vAlANL_T"
   },
   "outputs": [],
   "source": [
    "class UnFlatten(nn.Module):\n",
    "    def forward(self, input):\n",
    "        return input.view(input.size(0), 32, 14, 14)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "Collapsed": "false",
    "id": "hb_WB_CANL_T"
   },
   "outputs": [],
   "source": [
    "class Autoencoder(nn.Module):\n",
    "    def __init__(self, image_channels=3, z_dim=10):\n",
    "        super(Autoencoder, self).__init__()\n",
    "        self.encoder = nn.Sequential(\n",
    "            nn.Conv2d(image_channels, 16, kernel_size=4, stride=2),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(16, 32, kernel_size=4, stride=2),\n",
    "            nn.ReLU(),\n",
    "            Flatten(),\n",
    "        \n",
    "            nn.Linear(6272, 256),\n",
    "            nn.Linear(256, z_dim)\n",
    "            )\n",
    "        \n",
    "        self.decoder = nn.Sequential(\n",
    "            nn.Linear(z_dim, 256),\n",
    "            nn.Linear(256, 6272),\n",
    "        \n",
    "            UnFlatten(),\n",
    "        \n",
    "            nn.ConvTranspose2d(32, 16, kernel_size=5, stride=2),\n",
    "            nn.ReLU(),\n",
    "        \n",
    "            nn.ConvTranspose2d(16, image_channels, kernel_size=4, stride=2),\n",
    "            nn.Sigmoid()\n",
    "        )\n",
    "        \n",
    "\n",
    "\n",
    "    def forward(self, x):\n",
    "        out =self.encoder(x)\n",
    "        out = self.decoder(out)\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "Collapsed": "false",
    "id": "tpFCYlgnNL_U"
   },
   "outputs": [],
   "source": [
    "image_channels = fixed_x.size(1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "Collapsed": "false",
    "id": "0yRzg7x7qI7_"
   },
   "outputs": [],
   "source": [
    "model = Autoencoder(image_channels=image_channels).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "JRRzzY_YRM3k"
   },
   "outputs": [],
   "source": [
    "pip install pytorch_model_summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "9uWE-TkARM5Z"
   },
   "outputs": [],
   "source": [
    "import pytorch_model_summary\n",
    "print(pytorch_model_summary.summary(model, torch.zeros(1, 3, 64, 64).to(device), show_input=False))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Qcb6KDZ1Z0Ot"
   },
   "outputs": [],
   "source": [
    "optimizer = torch.optim.Adam(params=model.parameters(), lr=1e-3)\n",
    "\n",
    "# set to training mode\n",
    "model.train()\n",
    "\n",
    "train_loss_avg = []\n",
    "\n",
    "print('Training ...')\n",
    "for epoch in range(num_epochs):\n",
    "    train_loss_avg.append(0)\n",
    "    num_batches = 0\n",
    "    \n",
    "    for image_batch, _ in train_loader:\n",
    "        \n",
    "        image_batch = image_batch.to(device)\n",
    "        \n",
    "        # autoencoder reconstruction\n",
    "        image_batch_recon = model(image_batch)\n",
    "        \n",
    "        # reconstruction error\n",
    "        loss = F.mse_loss(image_batch_recon, image_batch)\n",
    "        \n",
    "        # backpropagation\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        \n",
    "        # one step of the optmizer (using the gradients from backpropagation)\n",
    "        optimizer.step()\n",
    "        \n",
    "        train_loss_avg[-1] += loss.item()\n",
    "        num_batches += 1\n",
    "        \n",
    "    train_loss_avg[-1] /= num_batches\n",
    "    print('Epoch [%d / %d] average reconstruction error: %f' % (epoch+1, num_epochs, train_loss_avg[-1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Ux2D8WIxcUbM"
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "fig = plt.figure()\n",
    "plt.plot(train_loss_avg)\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Reconstruction error')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "OnxXBlaYfMmz"
   },
   "outputs": [],
   "source": [
    "def compare(x):\n",
    "    recon_x= model(x)\n",
    "    return torch.cat([x, recon_x])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "Collapsed": "false",
    "id": "F94x7AZsNL_W"
   },
   "outputs": [],
   "source": [
    "fixed_x = train_data[randint(1, 100)][0].unsqueeze(0)\n",
    "compare_x = compare(fixed_x.to(device))\n",
    "\n",
    "save_image(compare_x.data.cpu(), 'sample_image.png')\n",
    "display(Image('sample_image.png', width=300, unconfined=True))"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "name": "final_2021.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
